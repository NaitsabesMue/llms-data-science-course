\documentclass[aspectratio=169]{beamer}
\usepackage{beamerthemeAMU}
\usepackage{amsmath}
\usepackage{booktabs}
\usepackage{array}
\usepackage{listings}
\usepackage{hyperref}
\usepackage{tikz}
\usetikzlibrary{arrows.meta}

\title{Large Language Models in Data Science}
\subtitle{Week 1: Concepts, Architecture, Motivation}
\author{Sebastian Mueller}
\institute{Aix-Marseille Universit\'e}
\date{2025-2026}

% Listing style for small code snippets
\lstset{basicstyle=\ttfamily\small,keywordstyle=\color{AMU_dk2}\bfseries,commentstyle=\itshape\color{gray},showstringspaces=false,columns=flexible}

\begin{document}

\begin{frame}[plain]
  \titlepage
\end{frame}

\begin{frame}{Session Overview}
  \begin{columns}[T,onlytextwidth]
    \begin{column}{0.55\linewidth}
      \textbf{Lecture (1.5h)}
      \begin{enumerate}
        \item Why study LLMs now?
        \item From words to tokens
        \item Transformer architecture
        \item Training and inference
        \item Capabilities, limits, ecosystem
      \end{enumerate}
    \end{column}
    \begin{column}{0.4\linewidth}
      \textbf{Lab (1.5h)}
      \begin{itemize}
        \item Tokenize and embed real text
        \item Inspect attention weights with HuggingFace
        \item Call OpenAI GPT models and estimate cost
        \item Bonus: visualize attention with BERTviz
      \end{itemize}
    \end{column}
  \end{columns}
\end{frame}

\section{Why Study LLMs?}

\begin{frame}{Motivation: Data Science is Changing}
  \begin{itemize}
    \item Analysts increasingly converse with their tools instead of writing boilerplate code.
    \item Reports, dashboards, and even experiments are drafted by generative models.
    \item Modern data workflows combine structured data with unstructured documents, code, and conversations.
    \item Understanding LLMs helps us design safer, more efficient assistants rather than treating them as black boxes.
  \end{itemize}
\end{frame}

\begin{frame}{What is a Large Language Model?}
  \begin{itemize}
    \item \textbf{Definition}: A neural network trained on massive text corpora to model the probability of token sequences.
    \item Given tokens $x_1, \dots, x_{t-1}$, the model estimates
      \[
        P(x_t \mid x_1, \dots, x_{t-1}).
      \]
    \item During inference the model samples one token at a time, feeding each prediction back as context.
    \item LLMs power question answering, summarization, code generation, and dialogue systems across industry.
  \end{itemize}
\end{frame}

\section{From Words to Tokens}

\begin{frame}{Motivation: Machines Need Numbers}
  \begin{itemize}
    \item Neural networks operate on numbers, not raw strings.
    \item Preprocessing must map text to numeric inputs while preserving meaning and structure.
    \item Tokenization, vocabularies, and embeddings create that bridge.
  \end{itemize}
\end{frame}

\begin{frame}{Core Concepts}
  \begin{columns}[T,onlytextwidth]
    \begin{column}{0.53\linewidth}
      \begin{block}{1. Token}
        Unit of text fed to the model. Depending on the tokenizer it may be a word, subword, character, or byte.
      \end{block}
      \begin{block}{2. Tokenization}
        Procedure that converts raw text into a sequence of tokens. 
      \end{block}
      \begin{block}{3. Vocabulary}
        Finite set of tokens the model knows. Typical vocabularies contain $V \approx 50{,}000$ tokens.
      \end{block}
    \end{column}
    \begin{column}{0.42\linewidth}
      \begin{exampleblock}{Example: ``Prediction''}
        \begin{tabular}{@{}ll@{}}
          Raw text & \texttt{Prediction} \\
          Tokens & \texttt{["Pred", "iction"]} \\
          Ids & \texttt{[4792, 1526]} \\
        \end{tabular}
        Subword splits let the model handle rare words by composing common pieces.
      \end{exampleblock}
    \end{column}
  \end{columns}
\end{frame}

\begin{frame}{Embeddings: Turning Tokens into Vectors}
  \begin{itemize}
    \item \textbf{Embedding matrix} $E \in \mathbb{R}^{V \times d}$ maps token ids to $d$-dimensional vectors.
    \item For token id $x_i$, lookup yields $\mathbf{e}_i = E[x_i] \in \mathbb{R}^d$.
    \item Embeddings capture semantic similarity: similar words live near each other in vector space.
    \item These vectors are the starting point for all subsequent Transformer computations.
  \end{itemize}
\end{frame}


\section{Transformer Architecture}

\begin{frame}{Motivation: Beyond Recurrent Networks}
  \begin{itemize}
    \item Recurrent Neural Networks process tokens sequentially, limiting parallelism and context length.
    \item Transformers (Vaswani et al., 2017) introduced self-attention, enabling long-range dependencies and scalable training.
    \item Result: state-of-the-art across NLP tasks, vision-language models, and code generation.
  \end{itemize}
\end{frame}

\begin{frame}{The Transformer at a Glance}
  \begin{itemize}
    \item \textbf{Encoder--Decoder (original paper)}: encoder builds contextual representations; decoder attends to both past outputs (self-attention) and encoder outputs (cross-attention).
    \item \textbf{Decoder-only (GPT family)}: uses only causal self-attention with a mask to prevent peeking ahead; simpler and efficient for generation.
    \item Common building blocks across both: embeddings + positional info, scaled dot-product attention, multi-head projections, residual connections, layer normalization, feed-forward networks.
  \end{itemize}
\end{frame}

\begin{frame}{Embeddings + Positional Information}
  \begin{itemize}
    \item Tokens map to vectors via an embedding matrix $E$; for id $x_i$, we get $\mathbf{e}_i = E[x_i]$.
    \item Positions are injected so the model can distinguish order: either \emph{sinusoidal} (deterministic) or \emph{learned} (adapted during training).
    \item The per-token input to the first layer is $\mathbf{h}_i^{(0)} = \mathbf{e}_i + \mathbf{p}_i$.
    \item \textbf{Why?} Self-attention by itself is permutation-invariant; positions restore sequence meaning (``dog bites man'' vs. ``man bites dog'').
  \end{itemize}
\end{frame}

\begin{frame}{Scaled Dot-Product Attention (Step-by-Step)}
  \begin{enumerate}
    \item Project inputs into queries $Q$, keys $K$, values $V$.
    \item Compute similarity $QK^T / \sqrt{d_k}$ to compare each token to every other token.
    \item Apply softmax to obtain attention weights (non-negative, sum to 1).
    \item Multiply by $V$ to aggregate content into contextualized outputs.
  \end{enumerate}
  \vspace{0.5em}
  \textbf{Why these choices?}
  \begin{itemize}
    \item \emph{Scaling} by $\sqrt{d_k}$ prevents softmax saturation for large key dimensions, stabilizing gradients.
    \item \emph{Causal masks} (decoder-only) enforce next-token prediction by zeroing out future positions.
    \item \emph{Cost}: pairwise comparisons across tokens yield $\mathcal{O}(n^2)$ operations per layer.
  \end{itemize}
\end{frame}

\begin{frame}{Multi-Head Attention (Why and How)}
  \begin{itemize}
    \item \textbf{Why}: different heads specialize in different relations (syntax, long-range dependencies, entity linking), improving expressiveness.
    \item \textbf{How}: run attention $h$ times with independent projections; concatenate head outputs and project back to the model dimension.
    \item \textbf{Benefit}: multiple perspectives in parallel, while each head uses lower-dimensional projections for efficiency.
  \end{itemize}
\end{frame}

\begin{frame}{Residual Connections and Layer Normalization}
  \begin{itemize}
    \item Add \& Norm around attention and feed-forward sublayers improves gradient flow and stabilizes deep networks.
    \item Residuals preserve information from earlier layers; normalization reduces internal covariate shift.
    \item \textbf{Why}: without these, very deep stacks are hard to train and prone to divergence.
  \end{itemize}
\end{frame}

\begin{frame}{Position-wise Feed-Forward Networks}
  \begin{itemize}
    \item Two linear layers with a nonlinearity (e.g., GELU/ReLU) applied independently to each token position.
    \item Acts like feature mixing per position after attention blends information across positions.
    \item \textbf{Why}: adds nonlinear capacity to transform representations beyond weighted averaging.
  \end{itemize}
\end{frame}

\begin{frame}{Stacking Layers and Depth}
  \begin{itemize}
    \item Repeating blocks (attention $\rightarrow$ FFN) yields deep models with large effective receptive fields.
    \item Deeper stacks capture hierarchical patterns, from local syntax to global discourse.
    \item Practical note: depth, width, and head count are trade-offs among quality, memory, and speed.
  \end{itemize}
\end{frame}

\begin{frame}{Decoder-Only vs. Encoder--Decoder}
  \begin{columns}[T,onlytextwidth]
    \begin{column}{0.48\linewidth}
      \textbf{Decoder-Only (GPT)}
      \begin{itemize}
        \item Causal self-attention with masks; generates left-to-right.
        \item Simpler architecture for pure generation and instruction following.
      \end{itemize}
    \end{column}
    \begin{column}{0.48\linewidth}
      \textbf{Encoder--Decoder (T5/Seq2Seq)}
      \begin{itemize}
        \item Encoder builds source representation; decoder performs masked self-attention + cross-attention to encoder.
        \item Well-suited for translation and general seq2seq tasks.
      \end{itemize}
    \end{column}
  \end{columns}
\end{frame}

\begin{frame}{Attention Patterns You Might See}
  \begin{itemize}
    \item Local heads focusing on nearby tokens (syntax, agreement).
    \item Global heads tracking key tokens (names, topics, brackets).
    \item Positional or delimiter-sensitive heads attending to special tokens.
    \item These patterns often emerge without explicit supervision.
  \end{itemize}
\end{frame}

\begin{frame}{Visual Walkthrough: Illustrated Transformer}
  \begin{itemize}
    \item Highly recommended: \href{https://jalammar.github.io/illustrated-transformer/}{The Illustrated Transformer (Jay Alammar)} for step-by-step visuals.
    \item Use in class to align equations with diagrams and build intuition.
  \end{itemize}
\end{frame}

\begin{frame}{LLM Model Timeline (2000--2024)}
  \centering
  \colorlet{colDec}{AMU_dk2}
  \colorlet{colEnc}{AMU_acc5}
  \colorlet{colED}{AMU_lt2!85!AMU_acc2}
  \colorlet{colNon}{AMU_acc4!60!AMU_lt1}

  \begin{tikzpicture}[>=Stealth, thick]
    \tikzset{event/.style={draw=AMU_dk1, rounded corners=2pt, inner xsep=4pt, inner ysep=2pt, font=\footnotesize, align=center}}

    % Baseline: dotted before 2017, solid afterwards
    \draw[dotted, AMU_acc4!80!AMU_dk1] (0,0) -- (2.6,0);
    \draw[->, AMU_dk1] (2.6,0) -- (10,0);

    % Ticks and labels
    \foreach \x/\label in {
      0/{$\sim$2000},
      2.6/2013,
      3.8/2017,
      4.6/2018,
      5.4/2019,
      6.2/2020,
      7.0/2021,
      7.8/2022,
      8.6/2023,
      9.4/2024
    }{
      \draw (\x,0) -- (\x,0.18);
      \node[below=2pt, font=\scriptsize] at (\x,0) {\label};
    }

    % Events
    % Non-transformer
    \draw (0,0) -- (0,0.9);
    \node[event, fill=colNon, anchor=south] at (0,0.9) {Bag-of-words};

    \draw (2.6,0) -- (2.6,1.2);
    \node[event, fill=colNon, anchor=south] at (2.6,1.2) {word2vec};

    \draw (3.8,0) -- (3.8,+0.9);
    \node[event, fill=colNon, anchor=north] at (3.8,+0.9) {Attention};

    % Encoder-only
    \draw (4.5,0) -- (4.5,1.1);
    \node[event, fill=colEnc, anchor=south] at (4.5,1.1) {BERT};

    \draw (5.3,0) -- (5.3,1.7);
    \node[event, fill=colEnc, anchor=south] at (5.3,1.7) {RoBERTa};

    \draw (5.5,0) -- (5.5,2.25);
    \node[event, fill=colEnc, anchor=south] at (5.5,2.25) {DistilBERT};

    % Decoder-only
    \draw (4.8,0) -- (4.8,-1.3);
    \node[event, fill=colDec, anchor=north] at (4.8,-1.3) {GPT};

    \draw (5.4,0) -- (5.4,-1.1);
    \node[event, fill=colDec, anchor=south] at (5.4,-1.1) {GPT-2};

    \draw (6.2,0) -- (6.2,-1.8);
    \node[event, fill=colDec, anchor=south] at (6.2,-1.8) {GPT-3};

    \draw (8.4,0) -- (8.4,1.6);
    \node[event, fill=colDec, anchor=south] at (8.4,1.6) {ChatGPT};

    \draw (8.9,0) -- (8.9,1.0);
    \node[event, fill=colDec, anchor=south] at (8.9,1.0) {GPT-4};

    % Encoder-decoder
    \draw (6.4,0) -- (6.4,1.0);
    \node[event, fill=colED, anchor=north] at (6.4,1.0) {T5};

    \draw (7.0,0) -- (7.0,1.0);
    \node[event, fill=colED, anchor=south] at (7.0,1.0) {Switch};

    \draw (8.3,0) -- (8.3,-0.8);
    \node[event, fill=colED, anchor=north] at (8.3,-0.8) {Flan-T5};

    % Legend
    \begin{scope}[shift={(0,-2.2)}]
      \node[event, fill=colDec] at (1.6,0) {\scriptsize Decoder-only};
      \node[event, fill=colEnc] at (4.8,0) {\scriptsize Encoder-only};
      \node[event, fill=colED] at (8.0,0) {\scriptsize Encoder--decoder};
      \node[event, fill=colNon] at (11.2,0) {\scriptsize Non-transformer models};
    \end{scope}
  \end{tikzpicture}
\end{frame}

\begin{frame}{Self-Attention: What and Why}
  \begin{columns}[T,onlytextwidth]
    \begin{column}{0.56\linewidth}
      \begin{itemize}
        \item \textbf{What}: Each token forms \emph{queries}, \emph{keys}, and \emph{values}: $Q = HW_Q$, $K = HW_K$, $V = HW_V$.
        \item \textbf{Computation}: 
          \[ \text{Attention}(Q,K,V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V. \]
        \item \textbf{Why softmax?} Turns similarity scores into a probability distribution to weight values.
        \item \textbf{Why scale by $\sqrt{d_k}$?} Prevents saturation of softmax for large $d_k$, stabilizing training.
        \item \textbf{Why causal masks?} Decoder LMs must not see future tokens; mask enforces next-token prediction.
        \item \textbf{Cost intuition}: pairwise comparisons across tokens $\Rightarrow$ $\mathcal{O}(n^2)$ per layer.
      \end{itemize}
    \end{column}
    \begin{column}{0.4\linewidth}
      \begin{exampleblock}{Tiny intuition}
        'The cat sat' $\to$ stronger weights to nearby/related tokens (e.g., 'cat' when processing 'sat').
      \end{exampleblock}
    \end{column}
  \end{columns}
\end{frame}

\begin{frame}{Multi-Head Attention and Positional Encoding}
  \begin{itemize}
    \item \textbf{Why multi-head?} Different heads specialize (syntax, co-reference, long vs. short-range) via different projections.
    \item Implementation: run attention $h$ times with different projections; concatenate and project back to model dimension.
    \item \textbf{Why positional encodings?} Attention is order-agnostic; positions distinguish 'dog bites man' vs. 'man bites dog'.
    \item Options: sinusoidal (deterministic) vs. learned/rotary (adapt to data, help long context).
  \end{itemize}
\end{frame}
\begin{frame}{Interactive: Transformer Explainer (Polo Club)}
  Explore attention live: \href{https://poloclub.github.io/transformer-explainer/}{poloclub.github.io/transformer-explainer}
\end{frame}

\begin{frame}{Transformer Decoder Block}
  \begin{itemize}
    \item Masked multi-head self-attention (prevents peeking at future tokens).
    \item Add \& norm (residual connection + layer normalization).
    \item Position-wise feed-forward network (two linear layers with non-linearity).
    \item Second add \& norm.
    \item Stack $L$ identical decoder blocks to build deep models (GPT-3: $L=96$ for the largest variant).
  \end{itemize}
\end{frame}

\section{Training and Inference}

\begin{frame}{Pretraining Objective}
  \begin{itemize}
    \item Learn next-token prediction by minimizing cross-entropy loss:
      \[
        \mathcal{L} = -\sum_{t} \log P(x_t \mid x_{<t}).
      \]
    \item Trained on internet-scale corpora combining code, books, documentation, and web data (hundreds of billions of tokens).
    \item Optimization tricks: AdamW optimizer, learning-rate warmup, cosine decay, gradient checkpointing, large batch parallelism.
  \end{itemize}
\end{frame}

\begin{frame}{Fine-Tuning and Alignment}
  \begin{itemize}
    \item \textbf{Supervised fine-tuning (SFT)} adapts the model on curated instruction-response pairs.
    \item \textbf{Reinforcement Learning from Human Feedback (RLHF)} trains a reward model to prefer helpful, harmless answers.
    \item Emerging alternatives: Direct Preference Optimization (DPO), Kahneman-Tversky Optimization (KTO), RAFT.
    \item Goal: steer base models toward desired tone, task adherence, and safety constraints.
  \end{itemize}
\end{frame}

\begin{frame}{Inference and Decoding Strategies}
  \begin{itemize}
    \item Generation proceeds token by token; decoding choices shape behavior.
    \item \textbf{Temperature}: scales logits to control randomness.
    \item \textbf{Top-$k$} and \textbf{top-$p$ (nucleus)} sampling: restrict candidate tokens to most probable set.
    \item \textbf{Max tokens}: upper bound on generated length; context window includes prompt + output.
    \item Engineering considerations: caching key/value tensors, speculative decoding, batching requests.
  \end{itemize}
\end{frame}

\section{Capabilities and Limitations}

\begin{frame}{What LLMs Do Well}
  \begin{itemize}
    \item Few-shot learning: adapt to new tasks using examples in the prompt.
    \item Chain-of-thought prompting: articulate intermediate reasoning steps.
    \item Code generation and refactoring for Python, SQL, and beyond.
    \item Summarization, translation, explanation across domains.
    \item Data-wrangling assistance: regex creation, feature brainstorming, doc parsing.
  \end{itemize}
\end{frame}

\begin{frame}{Limitations and Risks}
  \begin{itemize}
    \item \textbf{Hallucinations}: confident but fabricated answers without grounding.
    \item \textbf{Context window limits}: finite memory (e.g., 8k--200k tokens) affects long documents.
    \item \textbf{Bias and toxicity}: inherited from training data, requiring audits and guardrails.
    \item \textbf{Cost and latency}: API tokens and GPU serving are not free; optimize prompts and batching.
    \item \textbf{No persistent memory}: model forgets state across sessions unless you build storage.
  \end{itemize}
\end{frame}

\section{Ecosystem and Access}

\begin{frame}{Where to Get Models}
  \begin{columns}[T,onlytextwidth]
    \begin{column}{0.48\linewidth}
      \begin{tabular}{@{}lll@{}}
        \toprule
        Provider & Example Models & Access \\
        \midrule
        OpenAI & GPT-3.5, GPT-4o & Hosted API \\
        Anthropic & Claude 3 & Hosted API \\
        Meta & LLaMA 2/3 & Open weights \\
        Mistral & Mixtral, Mistral 7B & Open weights \\
        HuggingFace & Falcon, Phi-2, etc. & Hub + Transformers \\
        \bottomrule
      \end{tabular}
    \end{column}
    \begin{column}{0.45\linewidth}
      \textbf{Choosing a path}
      \begin{itemize}
        \item Hosted APIs: fast start, managed infrastructure, pay per token.
        \item Open weights: more control, but you manage serving, fine-tuning, and safety layers.
      \end{itemize}
    \end{column}
  \end{columns}
\end{frame}

\begin{frame}[fragile]{Calling an API}
  \begin{lstlisting}[language=Python]
from openai import OpenAI
client = OpenAI()

resp = client.chat.completions.create(
    model="gpt-4o-mini",
    messages=[
        {"role": "system", "content": "You are a data assistant."},
        {"role": "user", "content": "Tell me a joke about matrix inversion."}
    ],
    temperature=0.6,
    max_tokens=120,
)
print(resp.choices[0].message.content)
  \end{lstlisting}
\end{frame}

\begin{frame}{Token Economics}
  \begin{itemize}
    \item Providers charge per 1,000 tokens (roughly 750 words). Both input (prompt) and output tokens count.
    \item Example: if input = 500 tokens and output = 200 tokens at $\$0.002$ per 1K tokens, cost $\approx \$0.0014$.
    \item Track usage: log prompts, responses, and costs for transparency and budgeting.
  \end{itemize}
\end{frame}

\section{Lab Preview}

\begin{frame}{Lab Roadmap (3 Hours)}
  \begin{enumerate}
    \item Inspect tokenization and context length on sample datasets.
    \item Use HuggingFace `AutoModel` to extract self-attention scores.
    \item Call OpenAI API to summarize a dataset description; log latency and token usage.
    \item Estimate cost of common analytics queries; discuss caching strategies.
    \item Bonus: visualize attention patterns with BERTviz or BertViz (for encoder models).
  \end{enumerate}
\end{frame}

\section{Wrap-Up}

\begin{frame}{Key Takeaways}
  \begin{itemize}
    \item LLMs model $P(x_t \mid x_{<t})$ and rely on tokenization, embeddings, and Transformers to operate.
    \item Self-attention + positional encodings enable parallel processing and long-range context.
    \item Training spans massive pretraining, then alignment (SFT, RLHF, DPO) for safety and usefulness.
    \item Real-world use demands awareness of strengths, limitations, providers, and cost models.
    \item Lab will reinforce these concepts through hands-on tokenization and API exercises.
  \end{itemize}
\end{frame}

\begin{frame}{Further Reading}
  \begin{tabular}{@{}lp{6.8cm}@{}}
    Concept & Reference / Motivation \\
    \midrule
    Transformer & \href{https://jalammar.github.io/illustrated-transformer/}{Jay Alammar, Illustrated Transformer} -- visual intuition. \\
    Tokenization & \href{https://github.com/openai/tiktoken}{OpenAI tiktoken repo} -- understand token counts and cost. \\
    Training & \href{https://arxiv.org/abs/2005.14165}{GPT-3 Paper} -- scaling laws and setup. \\
    APIs & \href{https://platform.openai.com/docs}{OpenAI Documentation} -- production usage patterns. \\
    Fine-tuning & \href{https://huggingface.co/course}{HuggingFace Course} -- hands-on tutorials and tooling. \\
  \end{tabular}
\end{frame}

\begin{frame}{Looking Ahead}
  \begin{itemize}
    \item Next week: Prompt engineering and evaluation patterns.
    \item Prepare API keys, review prompt design guide, and collect example tasks you want to automate.
    \item Questions? Reach out on the course Slack before lab day.
  \end{itemize}
\end{frame}

\end{document}
