{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 1 Lab: Exploring Tokenization\n",
    "\n",
    "This notebook mirrors the lecture storyline:\n",
    "\n",
    "1. Break text into whitespace tokens and build a vocabulary.\n",
    "2. Turn documents into bag-of-words vectors.\n",
    "3. Compare modern subword tokenizers on real data.\n",
    "4. Generate dense embeddings to reason about word neighbours.\n",
    "5. (Optional) Call a hosted model on Hugging Face Inference for experimentation.\n",
    "\n",
    "The lab is self-contained—run the cells in order and record your observations in the reflection prompts at the end."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Setup\n",
    "\n",
    "Create a dedicated Python environment for this lab session, register it as a Jupyter kernel, and install the required packages:\n",
    "\n",
    "```bash\n",
    "python3 -m venv venvLLMDS\n",
    "source venvLLMDS/bin/activate  # Windows PowerShell: .\\venvLLMDS\\Scripts\\Activate.ps1\n",
    "pip install --upgrade pip\n",
    "pip install ipykernel\n",
    "ipython kernel install --user --name=venvLLMDS\n",
    "pip install --upgrade datasets transformers torch huggingface-hub pandas plotly\n",
    "```\n",
    "\n",
    "Make sure your VS Code / Jupyter session uses the newly created `venvLLMDS` kernel before running the cells—the rest of the notebook assumes that environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hugging Face token (optional)\n",
    "\n",
    "1. Create a free account at [huggingface.co](https://huggingface.co/).\n",
    "2. Generate a new token under **Settings → Access Tokens** (select the default `read` scope).\n",
    "3. Store it securely:\n",
    "   - On macOS/Linux: `echo 'HF_TOKEN=hf_your_token_here' >> .env`\n",
    "   - On Windows PowerShell: `Set-Content -Path .env -Value 'HF_TOKEN=hf_your_token_here'`\n",
    "4. Restart the notebook or run `load_dotenv()` so the token is available for the optional API cell.\n",
    "\n",
    "If you skip this step the notebook will still run; only the hosted inference demo is disabled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import os\n",
    "from collections import Counter\n",
    "from typing import Iterable\n",
    "\n",
    "from IPython.display import HTML, display\n",
    "import re\n",
    "\n",
    "import plotly.express as px\n",
    "import plotly.io as pio\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "try:\n",
    "    from huggingface_hub import InferenceClient\n",
    "except ImportError:  # pragma: no cover - optional dependency\n",
    "    InferenceClient = None\n",
    "\n",
    "HF_TOKEN = os.getenv('HF_TOKEN') or os.getenv('HUGGINGFACEHUB_API_TOKEN')\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Whitespace tokenization → vocabulary\n",
    "\n",
    "The lecture introduced tokenization by simply splitting text on whitespace. Recreate that pipeline for two sentences and inspect the intermediate artifacts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we dive into counting tokens, we define two short example sentences—one about robots, one about drones. Lowercasing plus `split()` mimics the lecture’s whitespace tokenizer and lets us build a small lookup table of positions versus tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = {\n",
    "    \"robotics\": \"This is a smart robot exploring language.\",\n",
    "    \"drones\": \"My agile drone is also very smart.\"\n",
    "}\n",
    "\n",
    "whitespace_tokens = {\n",
    "    name: sentence.lower().split()\n",
    "    for name, sentence in sentences.items()\n",
    "}\n",
    "\n",
    "token_table = pd.DataFrame(\n",
    "    [(name, i, token) for name, tokens in whitespace_tokens.items() for i, token in enumerate(tokens)],\n",
    "    columns=[\"sentence\", \"position\", \"token\"]\n",
    ")\n",
    "\n",
    "token_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To sanity-check the tokenization, the next cell arranges both token lists side-by-side. Empty cells simply indicate that one sentence ended sooner; this makes it easy to scan where the vocab overlaps or diverges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick side-by-side look at whitespace tokenization\n",
    "comparison = (pd.DataFrame.from_dict(whitespace_tokens, orient='index')\n",
    "              .T.reset_index(drop=False)\n",
    "              .rename(columns={'index': 'position'}))\n",
    "comparison.fillna('', inplace=True)\n",
    "comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a vocabulary and basic statistics\n",
    "vocabulary = sorted({token for tokens in whitespace_tokens.values() for token in tokens})\n",
    "vocab_counts = Counter(token for tokens in whitespace_tokens.values() for token in tokens)\n",
    "\n",
    "print(f\"Vocabulary size: {len(vocabulary)}\")\n",
    "pd.DataFrame({\"token\": vocabulary, \"frequency\": [vocab_counts[tok] for tok in vocabulary]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Manual bag-of-words vectors\n",
    "\n",
    "Bag-of-words encodes each document by counting occurrences from the shared vocabulary. This recreates the slide that mapped the toy vocabulary to vectors of 0/1 counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bag_of_words(tokens: Iterable[str], vocab: list[str]) -> np.ndarray:\n",
    "    counts = Counter(tokens)\n",
    "    return np.array([counts.get(term, 0) for term in vocab], dtype=np.int32)\n",
    "\n",
    "bow_vectors = {\n",
    "    name: bag_of_words(tokens, vocabulary)\n",
    "    for name, tokens in whitespace_tokens.items()\n",
    "}\n",
    "\n",
    "bow_df = pd.DataFrame(bow_vectors, index=vocabulary)\n",
    "bow_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "- Add a third sentence and observe how the vocabulary and bag-of-words representations change.\n",
    "- Compute cosine similarity between the bag-of-words vectors to quantify overlap."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Subword tokenizers on a dataset\n",
    "\n",
    "Whitespace tokenization breaks down for larger corpora. Use Hugging Face tokenizers to compare token counts on a small sample. This mirrors the lecture’s motivation for subword models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_names = [\"bert-base-uncased\", \"bert-base-cased\", \"gpt2\"]\n",
    "sentences = {\n",
    "    \"robotics\": \"This is a smart robot exploring language.\",\n",
    "    \"drones\": \"My agile drone is also very smart.\"\n",
    "}\n",
    "\n",
    "for model_name in model_names:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    print(f\"\\n=== Tokenization with {model_name} ===\")\n",
    "    \n",
    "    for label, sentence in sentences.items():\n",
    "        tokens = tokenizer.tokenize(sentence)\n",
    "        token_ids = tokenizer.encode(sentence)\n",
    "        print(f\"\\nSentence ({label}): {sentence}\")\n",
    "        print(\"Tokens:\", tokens)\n",
    "        print(\"Token IDs:\", token_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Data: a few longer texts to make tokenization visible\n",
    "ds = load_dataset(\"ag_news\", split=\"train[:200]\")\n",
    "texts = [row[\"text\"] for row in ds]\n",
    "\n",
    "examples = {\n",
    "    \"short\":  texts[5],\n",
    "    \"medium\": \" \".join(texts[20:22]),\n",
    "    \"long\":   \" \".join(texts[50:60]),\n",
    "}\n",
    "\n",
    "# 2) Tokenizers to compare\n",
    "model_names = [\n",
    "    \"Xenova/gpt-4\",                # byte-level BPE (Ġ = leading space)\n",
    "    \"bert-base-uncased\",   # WordPiece (## = continuation)\n",
    "    \"distilroberta-base\",  # BPE (similar to RoBERTa; shows Ġ/▁ depending on vocab)\n",
    "]\n",
    "\n",
    "tokenizers = {m: AutoTokenizer.from_pretrained(m, use_fast=True) for m in model_names}\n",
    "\n",
    "for model_name, tok in tokenizers.items():\n",
    "    print(f\"{model_name} → vocab size: {len(tok)}\")\n",
    "\n",
    "# 3) Helpers to prettify and color tokens\n",
    "def prettify_tokens(model_name, toks):\n",
    "    if \"gpt2\" in model_name or \"roberta\" in model_name:\n",
    "        # Show word boundaries explicitly; do not alter token boundaries\n",
    "        return [t.replace(\"Ġ\", \"␠\").replace(\"▁\", \"␠\") for t in toks]\n",
    "    # Keep BERT's '##' to illustrate subword continuation\n",
    "    return toks\n",
    "\n",
    "def colorize_tokens(tokens):\n",
    "    colors = [\"#eef6ff\", \"#FFD4CC\", \"#FABF8F\", \"#FFFE85\", \"#DCCBFE\"] # pastel colors\n",
    "    spans = []\n",
    "    for i, t in enumerate(tokens):\n",
    "        spans.append(\n",
    "            f'<span style=\"background:{colors[i % len(colors)]}; padding:2px; margin:1px; '\n",
    "            f'border-radius:3px; font-family:monospace;\">{t}</span>'\n",
    "        )\n",
    "    return \" \".join(spans)\n",
    "\n",
    "\n",
    "def show_tokenization(model_name, text):\n",
    "    tok = tokenizers[model_name]\n",
    "    ids = tok.encode(text, add_special_tokens=False)\n",
    "    toks = tok.convert_ids_to_tokens(ids)\n",
    "    pretty = prettify_tokens(model_name, toks)\n",
    "    html = (\n",
    "        f\"<h4>{model_name}</h4>\"\n",
    "        f\"<div style='font-family:system-ui; margin-bottom:6px;'><b>Tokens:</b></div>\"\n",
    "        f\"<div>{colorize_tokens(pretty)}</div>\"\n",
    "        f\"<div style='margin-top:6px; font-family:monospace;'>count = {len(toks)}</div>\"\n",
    "    )\n",
    "    return html, toks, ids\n",
    "\n",
    "# 4) Display: original text + per-model token view\n",
    "for label, text in examples.items():\n",
    "    display(HTML(f\"<h3>Example: {label}</h3><p style='line-height:1.4'>{text}</p>\"))\n",
    "    for m in model_names:\n",
    "        html, toks, ids = show_tokenization(m, text)\n",
    "        display(HTML(html))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us see how the number of tokens depends on the model with a more detailed study. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Load 1000 short news articles (text field)\n",
    "texts = [row[\"text\"] for row in load_dataset(\"ag_news\", split=\"train[:1000]\")]\n",
    "\n",
    "# 2) Choose tokenizers to compare\n",
    "model_names = [\"bert-base-uncased\", \"gpt2\", \"distilroberta-base\"]\n",
    "\n",
    "# 3) Compute token lengths per text per model\n",
    "records = []\n",
    "for model in model_names:\n",
    "    tok = AutoTokenizer.from_pretrained(model)\n",
    "    lengths = [len(tok(t).input_ids) for t in texts]\n",
    "    for L in lengths:\n",
    "        records.append({\"model\": model, \"tokens\": L})\n",
    "\n",
    "# 4) Build dataframe for plotting\n",
    "df = pd.DataFrame(records)\n",
    "\n",
    "# 5) Plot histogram overlay\n",
    "fig = px.histogram(\n",
    "    df,\n",
    "    x=\"tokens\",\n",
    "    color=\"model\",\n",
    "    barmode=\"overlay\",\n",
    "    nbins=40,\n",
    "    title=\"Token count distribution across models\"\n",
    ")\n",
    "fig.update_layout(bargap=0.05)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding overview and similarity\n",
    "\n",
    "We obtain sentence embeddings by mean pooling the token embeddings from a small transformer. To compare meanings, we use cosine similarity, which measures the angle between vectors and is scale‑invariant: s(u, v) = (u·v)/(|u||v|). We L2‑normalize embeddings before comparison to make cosine a simple dot product. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Offline fallback (no model download)\n",
    "\n",
    "If network/model download is unavailable, you can still practice the concept with a tiny toy example. Pick 6–8 words and make up 2‑D vectors (e.g., [[1,0], [0.9,0.1], [0,1], …]) that reflect rough similarity. Compute cosine similarities and nearest neighbors with NumPy. This mirrors the slide intuition: smaller angles → higher similarity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Dense embeddings and neighbours\n",
    "\n",
    "Use a sentence-transformer to obtain normalized embeddings and inspect cosine similarities, replicating the lecture’s intuition about “word neighbours.”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model_name = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "embedding_tokenizer = AutoTokenizer.from_pretrained(embedding_model_name)\n",
    "embedding_model = AutoModel.from_pretrained(embedding_model_name).to(DEVICE)\n",
    "\n",
    "def encode_texts(texts: list[str]) -> torch.Tensor:\n",
    "    encoded = embedding_tokenizer(texts, padding=True, truncation=True, return_tensors=\"pt\").to(DEVICE)\n",
    "    with torch.no_grad():\n",
    "        model_output = embedding_model(**encoded)\n",
    "    # Mean pooling then L2 normalize\n",
    "    embeddings = model_output.last_hidden_state.mean(dim=1)\n",
    "    embeddings = torch.nn.functional.normalize(embeddings, p=2, dim=1)\n",
    "    return embeddings.cpu()\n",
    "\n",
    "words = [\"cats\", \"dog\", \"puppy\", \"houses\", \"apple\", \"robot\", \"drone\"]\n",
    "embeddings = encode_texts(words)\n",
    "\n",
    "cos = torch.matmul(embeddings, embeddings.T)\n",
    "\n",
    "similarity_df = pd.DataFrame(cos.numpy(), index=words, columns=words)\n",
    "similarity_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion prompt\n",
    "\n",
    "Which pairs cluster together? Compare your findings with the “embedding neighbours” slide. How does this change if you swap in domain-specific words?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Optional: Hosted generation via Hugging Face Inference\n",
    "\n",
    "This mirrors the lecture’s “API call” segment without requiring OpenAI. If you set `HF_TOKEN`, the cell below sends a short prompt to the free-tier Inference API.\n",
    "\n",
    "> The free tier is rate-limited. Keep prompts short and cache responses for your report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if HF_TOKEN and InferenceClient is not None:\n",
    "    client = InferenceClient(model='mistralai/Mistral-7B-Instruct-v0.2', token=HF_TOKEN)\n",
    "    prompt = 'Summarise why subword tokenization is helpful for transformer models.'\n",
    "    response = client.text_generation(prompt, max_new_tokens=80, temperature=0.6)\n",
    "    print(response)\n",
    "elif InferenceClient is None:\n",
    "    print('Install huggingface-hub to enable the Inference API example.')\n",
    "else:\n",
    "    print('Skipping call: set HF_TOKEN to enable the Inference API example.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Reflection and deliverables\n",
    "\n",
    "- Compare whitespace and subword token counts—when does each strategy make sense?\n",
    "- How did embedding similarities align with your intuition from the lecture slide?\n",
    "- Include screenshots or tables of your experiments and discuss any surprises.\n",
    "- Make sure that every term, concept, function or package used does make sense to you and you are able to explain them. \n",
    "\n",
    "**Deliverables**\n",
    "\n",
    "- Short write-up summarising your findings (1 page).\n",
    "- CSV or markdown table logging tokenizer statistics.\n",
    "- Notes from at least one optional experiment (longer prompt, different model, custom vocabulary, etc.)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (venvLLMDS)",
   "language": "python",
   "name": "venvllmds"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
