{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 2 — Hugging Face & Transformers: Using Pretrained Models\n",
    "\n",
    "This notebook walks through the essentials for **using Hugging Face**. We treat:\n",
    "- Selecting checkpoints and using pipelines\n",
    "- Manual tokenization and model forward passes\n",
    "- Generation parameters and devices\n",
    "- Batching with `datasets`\n",
    "- Caching, offline mode, and revisions\n",
    "- Optional: Hosted Inference API\n",
    "\n",
    "Use the opportunity to play and vary the different parameters of the model to get an idea on their influence on the outcome.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fb09f7d",
   "metadata": {},
   "source": [
    "## Setup\n",
    "Install the core libraries (CPU by default). If you have a GPU, install the appropriate PyTorch build and optionally `bitsandbytes`.\n",
    "\n",
    "- Definition: An HF token is a personal key for accessing gated/private repos or hosted inference.\n",
    "- Why: Some models require accepting a license; hosted endpoints need to know who is calling.\n",
    "\n",
    "- Terminal: `pip install -U transformers datasets huggingface_hub accelerate safetensors`\n",
    "- GPU (optional): `pip install bitsandbytes` and a CUDA-enabled torch wheel.\n",
    "\n",
    "Authentication is only needed for gated/private repos or the hosted Inference API. You can either run `huggingface-cli login` or set `HF_TOKEN` in your environment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4c69e85",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import accelerate\n",
    "from transformers import (\n",
    "    pipeline,\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoModelForCausalLM,\n",
    ")\n",
    "from datasets import load_dataset\n",
    "\n",
    "HF_TOKEN = os.getenv('HF_TOKEN') or os.getenv('HUGGINGFACEHUB_API_TOKEN')\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else ('mps' if torch.backends.mps.is_available() else 'cpu')\n",
    "DEVICE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96244122",
   "metadata": {},
   "source": [
    "## 1) Pipelines: Quick Inference\n",
    "- Definition: A pipeline bundles the right tokenizer, model, and postprocessing for a task.\n",
    "- Why: It reduces moving parts so you can confirm the model works before customizing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e4fe40d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentiment analysis (binary SST-2)\n",
    "sent_clf = pipeline(\n",
    "    'text-classification',\n",
    "    model='distilbert-base-uncased-finetuned-sst-2-english',\n",
    "    device_map='auto'\n",
    ")\n",
    "sent_clf(['I love data!', 'This is terrible...'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e84b814",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill-mask\n",
    "mlm = pipeline('fill-mask', model='bert-base-uncased', device_map='auto')\n",
    "mlm('Marseille is the real [MASK] of France.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b204fdeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text generation (small model for speed)\n",
    "gen = pipeline('text-generation', model='gpt2', device_map='auto')\n",
    "gen('What is the capital of France?', max_new_tokens=40, do_sample=True, temperature=0.7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d16a0903",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "Use the version of gpt2 that was committed on Nov23, 20022, on Huggingface for the example above. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e401306e",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "- Try `zero-shot-classification` with `facebook/bart-large-mnli`.\n",
    "- Try `summarization` with `facebook/bart-large-cnn` on a paragraph.\n",
    "- Try `feature-extraction` with `sentence-transformers/all-MiniLM-L6-v2` and compute cosine similarity between two sentences.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a849294",
   "metadata": {},
   "source": [
    "## 2) Manual Tokenization + Model Forward\n",
    "\n",
    "### Understanding AutoModelForSequenceClassification\n",
    "- **What**: A model architecture for text classification with:\n",
    "  - Transformer backbone (e.g., BERT, DistilBERT)\n",
    "  - Classification head (linear layer) on top\n",
    "  - Output logits for each class (here: positive/negative sentiment)\n",
    "- **Why**: Perfect for tasks like sentiment analysis, topic classification, or intent detection\n",
    "\n",
    "In this example, we're doing binary sentiment classification:\n",
    "- Input: Text (e.g., movie review)\n",
    "- Output: Binary prediction (0 = negative, 1 = positive)\n",
    "- Model: DistilBERT fine-tuned on SST-2 (Stanford Sentiment Treebank)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fc2be59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up model and tokenizer for manual inference\n",
    "model_id = 'distilbert-base-uncased-finetuned-sst-2-english'\n",
    "tok = AutoTokenizer.from_pretrained(model_id)\n",
    "mdl = AutoModelForSequenceClassification.from_pretrained(model_id)\n",
    "\n",
    "# Manual inference\n",
    "texts = [\n",
    "    'I absolutely loved this movie!',\n",
    "    'The movie was okay; yet I can not say anything positive about it.',\n",
    "    'The plot was weak and boring.',\n",
    "    'The actors where neither good nor bad, just average.',\n",
    "]\n",
    "# Create batch\n",
    "batch = tok(texts, padding=True, truncation=True, return_tensors='pt')\n",
    "\n",
    "# Forward pass\n",
    "with torch.no_grad(): # disable gradient calculation for memory efficiency\n",
    "    out = mdl(**batch) # forward pass\n",
    "\n",
    "# Convert logits to probabilities\n",
    "    probs = out.logits.softmax(-1)\n",
    "    probs\n",
    "\n",
    "# Print results with class labels\n",
    "labels = ['negative', 'positive']\n",
    "for text, prob in zip(texts, probs):\n",
    "    pred_class = labels[prob.argmax().item()]\n",
    "    confidence = prob.max().item()\n",
    "    print(f\"\\nText: {text}\\nPrediction: {pred_class} ({confidence:.2%})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3afd2e36",
   "metadata": {},
   "source": [
    "### Understanding Text Generation with `generate()`\n",
    "- **What**: A method for auto-regressive text generation that:\n",
    "  - Takes an input prompt\n",
    "  - Generates new tokens one by one\n",
    "  - Uses different decoding strategies (greedy, beam, sampling)\n",
    "- **Why**: Control the trade-off between:\n",
    "  - Creativity vs determinism\n",
    "  - Diversity vs coherence\n",
    "  - Speed vs quality\n",
    "\n",
    "Key parameters explained:\n",
    "- `max_new_tokens`: Maximum length of generated text\n",
    "- `temperature`: Controls randomness (higher = more creative)\n",
    "- `top_p`: Nucleus sampling threshold (cumulative probability)\n",
    "- `top_k`: Limits vocabulary to k most likely tokens\n",
    "- `num_beams`: Number of parallel sequences to explore\n",
    "- `repetition_penalty`: Discourages word repetition\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f943b080",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up model and tokenizer\n",
    "lm_id = 'gpt2'  # small demo model\n",
    "tok_lm = AutoTokenizer.from_pretrained(lm_id)\n",
    "lm = AutoModelForCausalLM.from_pretrained(lm_id)\n",
    "\n",
    "# Prepare input\n",
    "prompt = 'What is so special about Marseille?'\n",
    "inputs = tok_lm(prompt, return_tensors='pt')\n",
    "\n",
    "# Generate with different settings\n",
    "with torch.no_grad():  # inference mode\n",
    "    # Conservative settings (more focused)\n",
    "    conservative = lm.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=64,\n",
    "        num_beams=5,\n",
    "        temperature=0.7,\n",
    "        repetition_penalty=1.2\n",
    "    )\n",
    "    \n",
    "    # Creative settings (more diverse)\n",
    "    creative = lm.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=64,\n",
    "        do_sample=True,\n",
    "        temperature=1.2,\n",
    "        top_p=0.92,\n",
    "        top_k=50\n",
    "    )\n",
    "\n",
    "# Print results\n",
    "print(\"Conservative output:\")\n",
    "print(tok_lm.decode(conservative[0], skip_special_tokens=True))\n",
    "print(\"\\nCreative output:\")\n",
    "print(tok_lm.decode(creative[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8bc0d61",
   "metadata": {},
   "source": [
    "## 3) Devices and Memory\n",
    "### Understanding Device Placement and Memory Options\n",
    "- **What**: Ways to optimize model loading and inference:\n",
    "  - `device_map=\"auto\"`: Smart layer distribution across hardware\n",
    "  - `dtype`: Numeric precision (float32, float16, bfloat16)\n",
    "  - Quantization: 8-bit or 4-bit weights (via `bitsandbytes`)\n",
    "- **Why**: Critical for:\n",
    "  - Fitting larger models in limited memory\n",
    "  - Speeding up inference\n",
    "  - Balancing speed vs accuracy\n",
    "\n",
    "### Key Concepts\n",
    "1. **Device Mapping**:\n",
    "   - `auto`: Automatic placement based on available memory\n",
    "   - `cpu`/`cuda`/`mps`: Manual device selection\n",
    "   - Split placement for very large models\n",
    "\n",
    "2. **Precision Options**:\n",
    "   - `float32`: Full precision, highest accuracy\n",
    "   - `float16`: Half precision, 2x memory savings\n",
    "   - `bfloat16`: Better numeric stability than float16\n",
    "\n",
    "3. **Quantization**:\n",
    "   - 8-bit: Good balance of speed/memory/accuracy\n",
    "   - 4-bit: Maximum memory savings\n",
    "   - Requires: `bitsandbytes` library\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "402ea341",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time, torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "def run_inference_benchmark(name, texts, devices, dtypes):\n",
    "    \"\"\"Benchmark model performance across devices and dtypes\"\"\"\n",
    "    rows = []\n",
    "    \n",
    "    for device in devices:\n",
    "        for dtype in dtypes:\n",
    "            # Skip unsupported combinations\n",
    "            if device == 'cpu' and dtype is torch.float16:\n",
    "                continue\n",
    "                \n",
    "            # Load model with specified precision\n",
    "            print(f\"\\nTesting {device} with {dtype}\")\n",
    "            tok = AutoTokenizer.from_pretrained(name)\n",
    "            mdl = AutoModelForSequenceClassification.from_pretrained(\n",
    "                name, \n",
    "                torch_dtype=dtype,\n",
    "                device_map='auto' if device == 'cuda' else None\n",
    "            )\n",
    "            if device != 'cuda':  # Only move to device if not using device_map='auto'\n",
    "                mdl.to(device)\n",
    "            \n",
    "            # Prepare input batch\n",
    "            batch = tok(texts, padding=True, truncation=True, return_tensors='pt')\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            \n",
    "            # Memory measurements\n",
    "            param_mb = sum(p.numel() * p.element_size() for p in mdl.parameters()) / 1e6\n",
    "            if device == 'cuda':\n",
    "                torch.cuda.reset_peak_memory_stats()\n",
    "                \n",
    "            # Warmup run\n",
    "            with torch.no_grad():\n",
    "                mdl(**batch)\n",
    "                \n",
    "            # Timed inference\n",
    "            t0 = time.perf_counter()\n",
    "            with torch.no_grad():\n",
    "                for _ in range(10):\n",
    "                    mdl(**batch)\n",
    "            dt = time.perf_counter() - t0\n",
    "            \n",
    "            # Record peak GPU memory if applicable\n",
    "            peak_mb = None\n",
    "            if device == 'cuda':\n",
    "                peak_mb = torch.cuda.max_memory_allocated() / 1e6\n",
    "                \n",
    "            rows.append((\n",
    "                device, \n",
    "                str(dtype).replace('torch.', ''),\n",
    "                round(param_mb, 1),\n",
    "                round(dt, 3),\n",
    "                None if peak_mb is None else round(peak_mb, 1)\n",
    "            ))\n",
    "            \n",
    "    return rows\n",
    "\n",
    "# Run benchmark\n",
    "name = 'distilbert-base-uncased-finetuned-sst-2-english'\n",
    "texts = ['STUDYING DATA SCIENCE IS FUN!'] * 32  # Multiple instances for better timing\n",
    "\n",
    "# Detect available devices\n",
    "devices = []\n",
    "if torch.cuda.is_available(): devices.append('cuda')\n",
    "if torch.backends.mps.is_available(): devices.append('mps')\n",
    "devices.append('cpu')\n",
    "\n",
    "# Test different precisions\n",
    "dtypes = [torch.float32, torch.float16]\n",
    "\n",
    "# Run and display results\n",
    "rows = run_inference_benchmark(name, texts, devices, dtypes)\n",
    "print('\\nBenchmark Results:')\n",
    "print(f\"{'Device':>8} {'Dtype':>8} {'ParamMB':>8} {'Seconds':>8} {'PeakMB':>8}\")\n",
    "print('-' * 45)\n",
    "for r in rows:\n",
    "    print(f\"{r[0]:>8} {r[1]:>8} {r[2]:>8.1f} {r[3]:>8.3f} {str(r[4]):>8}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f69dfb5c",
   "metadata": {},
   "source": [
    "## 4) Understanding Batch Processing with Hugging Face Transformers\n",
    "\n",
    "### Key Concepts\n",
    "- **What**: Processing multiple inputs simultaneously through the model\n",
    "- **Why**: Trade-off between throughput and memory usage\n",
    "  - ✓ Faster: Parallel processing of multiple inputs\n",
    "  - ✓ Efficient: Better GPU utilization\n",
    "  - × Memory: Higher RAM/VRAM usage due to padding\n",
    "  - × Latency: Must wait for entire batch to complete\n",
    "\n",
    "### Memory Considerations\n",
    "1. **Input Padding**:\n",
    "   - Batches must have uniform length\n",
    "   - Shorter sequences get padded to longest in batch\n",
    "   - More padding = more wasted memory\n",
    "\n",
    "2. **Activation Memory**:\n",
    "   - Scales with batch size\n",
    "   - Depends on model architecture\n",
    "   - Key limiting factor for large models\n",
    "\n",
    "3. **Optimal Batch Size**:\n",
    "   - Hardware dependent (GPU memory)\n",
    "   - Task dependent (sequence length)\n",
    "   - Model dependent (parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69f8911d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import time\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score\n",
    "import psutil\n",
    "\n",
    "def measure_memory_cpu():\n",
    "    \"\"\"Get current memory usage in MB\"\"\"\n",
    "    process = psutil.Process()\n",
    "    return process.memory_info().rss / 1024 / 1024\n",
    "\n",
    "def process_with_detailed_metrics(dataset, batch_size, model, tokenizer):\n",
    "    \"\"\"Process dataset with detailed per-batch metrics\"\"\"\n",
    "    predictions = []\n",
    "    memory_usage = []\n",
    "    batch_times = []\n",
    "    batch_accuracies = []\n",
    "    \n",
    "    # Record initial memory state\n",
    "    base_memory = measure_memory_cpu()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.reset_peak_memory_stats()\n",
    "        base_gpu_memory = torch.cuda.memory_allocated()\n",
    "    \n",
    "    with tqdm(range(0, len(dataset), batch_size), desc='Processing') as pbar:\n",
    "        for i in pbar:\n",
    "            batch = dataset[i:i + batch_size]\n",
    "            batch_t0 = time.perf_counter()\n",
    "            \n",
    "            # Process batch\n",
    "            inputs = tokenizer(\n",
    "                batch['text'],  # Using 'text' field from dataset\n",
    "                padding=True,\n",
    "                truncation=True,\n",
    "                max_length=512,\n",
    "                return_tensors='pt'\n",
    "            ).to(model.device)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                outputs = model(**inputs)\n",
    "                preds = outputs.logits.argmax(-1).cpu().tolist()\n",
    "                predictions.extend(preds)\n",
    "            \n",
    "            # Metrics\n",
    "            batch_time = time.perf_counter() - batch_t0\n",
    "            batch_times.append(batch_time)\n",
    "            \n",
    "            current_cpu_mem = measure_memory_cpu() - base_memory\n",
    "            if torch.cuda.is_available():\n",
    "                current_gpu_mem = torch.cuda.max_memory_allocated() / 1e6\n",
    "                total_mem = current_cpu_mem + current_gpu_mem\n",
    "            else:\n",
    "                total_mem = current_cpu_mem\n",
    "                \n",
    "            memory_usage.append(total_mem)\n",
    "            \n",
    "            # Update progress\n",
    "            pbar.set_postfix({\n",
    "                'Memory (MB)': f'{total_mem:.0f}',\n",
    "                'Time (s)': f'{batch_time:.3f}'\n",
    "            })\n",
    "    \n",
    "    return {\n",
    "        'predictions': predictions,\n",
    "        'batch_times': batch_times,\n",
    "        'memory_usage': memory_usage,\n",
    "        'avg_batch_time': np.mean(batch_times),\n",
    "        'avg_memory': np.mean(memory_usage),\n",
    "        'peak_memory': max(memory_usage)\n",
    "    }\n",
    "\n",
    "# Load test dataset\n",
    "dataset = load_dataset('imdb', split='test[:100]')  # Small subset for demo\n",
    "\n",
    "# Load model and tokenizer\n",
    "model_name = 'distilbert-base-uncased-finetuned-sst-2-english'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    model = model.cuda()\n",
    "\n",
    "# Test different batch sizes\n",
    "batch_sizes = [1, 4, 16, 32]\n",
    "results = {}\n",
    "\n",
    "for bs in batch_sizes:\n",
    "    print(f\"\\nTesting batch_size={bs}\")\n",
    "    results[bs] = process_with_detailed_metrics(dataset, bs, model, tokenizer)\n",
    "\n",
    "# Visualize results\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# Plot timing\n",
    "times = [r['avg_batch_time'] for r in results.values()]\n",
    "ax1.plot(batch_sizes, times, marker='o')\n",
    "ax1.set_xlabel('Batch Size')\n",
    "ax1.set_ylabel('Average Time per Batch (s)')\n",
    "ax1.set_title('Processing Time vs Batch Size')\n",
    "\n",
    "# Plot memory\n",
    "memory = [r['peak_memory'] for r in results.values()]\n",
    "ax2.plot(batch_sizes, memory, marker='o', color='orange')\n",
    "ax2.set_xlabel('Batch Size')\n",
    "ax2.set_ylabel('Peak Memory Usage (MB)')\n",
    "ax2.set_title('Memory Usage vs Batch Size')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bf7a67c",
   "metadata": {},
   "source": [
    "## 5) Revisions, Caching, and Offline\n",
    "- Definition: A checkpoint is a released model; a revision is an exact commit/tag.\n",
    "- Why: Pinning revisions and controlling caches ensures reproducibility on different machines.\n",
    "- Pin exact versions with `revision=` when calling `from_pretrained`.\n",
    "- Set cache directories with `HF_HOME` or `TRANSFORMERS_CACHE`.\n",
    "- Force offline mode with `HF_HUB_OFFLINE=1` (uses only local cache).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01c1f243",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: pinning a revision (replace with a real commit SHA/tag for production)\n",
    "tok_pinned = AutoTokenizer.from_pretrained('bert-base-uncased', revision='main')\n",
    "mdl_pinned = AutoModelForSequenceClassification.from_pretrained('distilbert-base-uncased-finetuned-sst-2-english', revision='main')\n",
    "\n",
    "# Where is the cache?\n",
    "print('HF_HOME =', os.getenv('HF_HOME'))\n",
    "print('TRANSFORMERS_CACHE =', os.getenv('TRANSFORMERS_CACHE'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eae6b11",
   "metadata": {},
   "source": [
    "## 6) Optional: Hosted Inference API\n",
    "- Definition: The Inference API is a managed endpoint for common tasks.\n",
    "- Why: Zero setup for quick demos or when you lack local GPU resources.\n",
    "Run inference on HF-hosted models (requires token; rate limits apply).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2da7cdba",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from huggingface_hub import InferenceClient\n",
    "    if HF_TOKEN:\n",
    "        client = InferenceClient(model='facebook/bart-large-cnn', token=HF_TOKEN)\n",
    "        s = client.summarization('''Large Language Models (LLMs) are a transformative technology in artificial intelligence, powering applications like chatbots, text generation, and automated analysis. These models, built on deep learning architectures, excel at understanding and generating human-like text by learning patterns from vast datasets. At their core, LLMs are neural networks trained on billions of words from diverse sources, such as books, websites, and social media, enabling them to capture the nuances of language, from grammar to context.\n",
    "The foundation of LLMs lies in the transformer architecture, introduced in 2017 with the paper \"Attention is All You Need.\" Transformers use mechanisms like self-attention to process input text, allowing the model to weigh the importance of each word relative to others in a sentence. This enables LLMs to handle long-range dependencies, making them adept at tasks like translation, summarization, and question-answering. Models like BERT, GPT, and Llama have pushed the boundaries of what machines can achieve, with each iteration scaling up in size and capability.\n",
    "Training an LLM involves feeding it massive text corpora and optimizing billions of parameters using techniques like supervised learning and reinforcement learning. The process is computationally intensive, requiring powerful GPUs or TPUs and significant energy resources. Once trained, LLMs can perform zero-shot or few-shot learning, meaning they can tackle tasks with little to no task-specific training, relying on their prelearned knowledge. For example, an LLM can generate a story from a prompt like \"Once upon a time\" or classify sentiment in a sentence without explicit retraining.\n",
    "LLMs are accessible through platforms like the Hugging Face Hub, which hosts pretrained models, datasets, and tools like the transformers library. This democratizes AI, allowing developers to use models like GPT-2 or DistilBERT for tasks such as text classification or generation without building from scratch. However, using LLMs responsibly requires understanding their limitations, including biases in training data, high computational costs, and potential security risks when loading untrusted models.\n",
    "Applications of LLMs span industries: they power virtual assistants, automate customer support, assist in coding, and enhance research by summarizing complex texts. Yet, challenges remain, including ensuring fairness, reducing environmental impact, and managing the risk of generating misleading information. As LLMs evolve, they promise to reshape how we interact with technology, making it critical to approach their development and use with care.\n",
    "In summary, LLMs represent a leap forward in AI, driven by transformers and massive datasets. Their ability to process and generate language has broad implications, but careful management is essential to harness their potential effectively..''')\n",
    "        print(s)\n",
    "    else:\n",
    "        print('HF_TOKEN not set; skipping hosted inference.')\n",
    "except Exception as e:\n",
    "    print('InferenceClient not available or error:', e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10869928",
   "metadata": {},
   "source": [
    "## Exercise — Model comparison and benchmarking\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05a52d5f",
   "metadata": {},
   "source": [
    "###  Task Description\n",
    "Create a comprehensive comparison of two sentiment classifiers using the IMDB dataset. Your analysis should include:\n",
    "\n",
    "#### Step 1: Model Selection  and Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "595744a6",
   "metadata": {},
   "source": [
    "Complete the setup part and logging of the used models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebb6705e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO import the necessary packages\n",
    "\n",
    "# Model configurations\n",
    "models = {\n",
    "    'distilbert': {\n",
    "        'name': 'distilbert-base-uncased-finetuned-sst-2-english',\n",
    "        'revision': 'main',  # Pin for reproducibility\n",
    "    },\n",
    "    'bert': {\n",
    "        'name': 'textattack/bert-base-uncased-SST-2',\n",
    "        'revision': 'main',  # Pin for reproducibility\n",
    "    }\n",
    "}\n",
    "\n",
    "# Print model architectures and sizes\n",
    "for model_key, config in models.items():\n",
    "    model = #TODO\n",
    "    n_params = sum(p.numel() for p in model.parameters())\n",
    "    print(f\"\\n{model_key.upper()} Architecture:\")\n",
    "    print(f\"Parameters: {n_params:,}\")\n",
    "    print(f\"Label mapping: {model.config.id2label}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63693eff",
   "metadata": {},
   "source": [
    "### Step 2: Data Loading and Processing\n",
    "Complete the dataset loading and preprocessing.\n",
    "Hint: Use `load_dataset('imdb')` and implement proper text cleaning if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b34aacb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import concatenate_datasets\n",
    "# Data loading template\n",
    "def load_and_preprocess_data(split='test', max_samples=1000):\n",
    "    \"\"\"Load and preprocess IMDB data\"\"\"\n",
    "    # Load dataset\n",
    "    dataset = load_dataset('imdb', split=split)\n",
    "\n",
    "   #TODO Separate positive and negative samples\n",
    "    \n",
    "\n",
    "    #TODO Take equal samples from both classes\n",
    "   \n",
    "    #TODO  Create a balanced dataset\n",
    "    balanced_dataset = #TODO\n",
    "\n",
    "    #TODO  Shuffle the balanced dataset\n",
    "    balanced_dataset = balanced_dataset.shuffle(seed=42)\n",
    "    return balanced_dataset\n",
    "\n",
    "# Load evaluation data\n",
    "eval_dataset = load_and_preprocess_data(split='test', max_samples=1000)\n",
    "\n",
    "# Verify the proportion of positive/negative samples\n",
    "labels = [example['label'] for example in eval_dataset]\n",
    "pos_ratio = sum(labels) / len(labels)\n",
    "print(f\"Positive samples: {pos_ratio:.2%}, Negative samples: {1 - pos_ratio:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ad2bcfb",
   "metadata": {},
   "source": [
    "### Step 3: Evaluation Function\n",
    "Reuse the benchmarking code from earlier and adapt it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae2dad14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model_name, dataset, batch_size=32, device='auto'):\n",
    "    \"\"\"Evaluate model performance and efficiency\"\"\"\n",
    "    # Determine device\n",
    "    if device == 'auto':\n",
    "        device = 'cuda' if torch.cuda.is_available() else ('mps' if torch.backends.mps.is_available() else 'cpu')\n",
    "    \n",
    "    # Load model and tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        model_name,\n",
    "        device_map='auto' if device == 'cuda' else None\n",
    "    )\n",
    "    if device != 'cuda':\n",
    "        model.to(device)\n",
    "    \n",
    "    # Initialize metrics\n",
    "    predictions = []\n",
    "    labels = []\n",
    "    batch_times = []\n",
    "    \n",
    "    # Process batches\n",
    "    for i in range(0, len(dataset), batch_size):\n",
    "        batch = dataset[i:i + batch_size]\n",
    "        batch_start = time.perf_counter()\n",
    "        \n",
    "        #TODO  Tokenize and prepare input\n",
    "        inputs = tokenizer(#TODO)\n",
    "        \n",
    "        # Get predictions\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "            preds = #TODO\n",
    "        \n",
    "        # Record metrics\n",
    "        predictions.extend(preds)\n",
    "        labels.extend(batch['label'])\n",
    "        batch_times.append(time.perf_counter() - batch_start)\n",
    "    \n",
    "    # Calculate metrics with full classification report\n",
    "    report = classification_report(\n",
    "            labels, \n",
    "            predictions, \n",
    "            output_dict=True,\n",
    "            zero_division=0,\n",
    "            target_names=['negative', 'positive']\n",
    "    )\n",
    "    \n",
    "    # Extract all relevant metrics\n",
    "    metrics = {\n",
    "        'accuracy': report['accuracy'],\n",
    "        'negative': report['negative'],  # Contains precision, recall, f1-score\n",
    "        'positive': report['positive'],  # Contains precision, recall, f1-score\n",
    "        'avg_time_per_batch': sum(batch_times) / len(batch_times),\n",
    "        'total_time': sum(batch_times)\n",
    "    }\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "def print_detailed_results(results):\n",
    "    \"\"\"Print detailed results for each model\"\"\"\n",
    "    for model_key, metrics in results.items():\n",
    "        print(f\"\\nResults for {model_key.upper()}:\")\n",
    "        print(f\"Accuracy: {metrics['accuracy']:.4f}\")\n",
    "        print(\"Class-wise Metrics:\")\n",
    "        for cls in ['negative', 'positive']:\n",
    "            print(f\"  {cls.capitalize()}:\")\n",
    "            print(f\"    Precision: {metrics[cls]['precision']:.4f}\")\n",
    "            print(f\"    Recall:    {metrics[cls]['recall']:.4f}\")\n",
    "            print(f\"    F1-Score:  {metrics[cls]['f1-score']:.4f}\")\n",
    "        print(f\"Average Time per Batch: {metrics['avg_time_per_batch']:.4f} seconds\")\n",
    "        print(f\"Total Evaluation Time: {metrics['total_time']:.4f} seconds\")\n",
    "\n",
    "# Test both models\n",
    "results = {}\n",
    "for model_key, config in models.items():\n",
    "    print(f\"\\nEvaluating {model_key}...\")\n",
    "    metrics = evaluate_model(\n",
    "        config['name'],\n",
    "        eval_dataset,\n",
    "        batch_size=32,\n",
    "        device='auto'\n",
    "    )\n",
    "    results[model_key] = metrics\n",
    "\n",
    "# Display results\n",
    "print_detailed_results(results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (venvLLMDS)",
   "language": "python",
   "name": "venvllmds"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
