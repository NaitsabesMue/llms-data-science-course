{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 2 — Hugging Face & Transformers: Using Pretrained Models\n",
    "\n",
    "This notebook walks through the essentials for **using Hugging Face**. We treat:\n",
    "- Selecting checkpoints and using pipelines\n",
    "- Manual tokenization and model forward passes\n",
    "- Generation parameters and devices\n",
    "- Batching with `datasets`\n",
    "- Caching, offline mode, and revisions\n",
    "- Optional: Hosted Inference API\n",
    "\n",
    "Use the opportunity to play and vary the different parameters of the model to get an idea on their influence on the outcome.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fb09f7d",
   "metadata": {},
   "source": [
    "## Setup\n",
    "Install the core libraries (CPU by default). If you have a GPU, install the appropriate PyTorch build and optionally `bitsandbytes`.\n",
    "\n",
    "- Definition: An HF token is a personal key for accessing gated/private repos or hosted inference.\n",
    "- Why: Some models require accepting a license; hosted endpoints need to know who is calling.\n",
    "\n",
    "- Terminal: `pip install -U transformers datasets huggingface_hub accelerate safetensors`\n",
    "- GPU (optional): `pip install bitsandbytes` and a CUDA-enabled torch wheel.\n",
    "\n",
    "Authentication is only needed for gated/private repos or the hosted Inference API. You can either run `huggingface-cli login` or set `HF_TOKEN` in your environment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4c69e85",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import accelerate\n",
    "from transformers import (\n",
    "    pipeline,\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoModelForCausalLM,\n",
    ")\n",
    "from datasets import load_dataset\n",
    "\n",
    "HF_TOKEN = os.getenv('HF_TOKEN') or os.getenv('HUGGINGFACEHUB_API_TOKEN')\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else ('mps' if torch.backends.mps.is_available() else 'cpu')\n",
    "DEVICE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96244122",
   "metadata": {},
   "source": [
    "## 1) Pipelines: Quick Inference\n",
    "- Definition: A pipeline bundles the right tokenizer, model, and postprocessing for a task.\n",
    "- Why: It reduces moving parts so you can confirm the model works before customizing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e4fe40d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentiment analysis (binary SST-2)\n",
    "sent_clf = pipeline(\n",
    "    'text-classification',\n",
    "    model='distilbert-base-uncased-finetuned-sst-2-english',\n",
    "    device_map='auto'\n",
    ")\n",
    "sent_clf(['I love data!', 'This is terrible...'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e84b814",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill-mask\n",
    "mlm = pipeline('fill-mask', model='bert-base-uncased', device_map='auto')\n",
    "mlm('Marseille is the real [MASK] of France.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b204fdeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text generation (small model for speed)\n",
    "gen = pipeline('text-generation', model='gpt2', device_map='auto')\n",
    "gen('Tell me a joke about the people of Marseille:', max_new_tokens=40, do_sample=True, temperature=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d16a0903",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "Use the version of gpt2 that was committed on Nov23, 20022, on Huggingface for the example above. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e401306e",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "- Try `zero-shot-classification` with `facebook/bart-large-mnli`.\n",
    "- Try `summarization` with `facebook/bart-large-cnn` on a paragraph.\n",
    "- Try `feature-extraction` with `sentence-transformers/all-MiniLM-L6-v2` and compute cosine similarity between two sentences.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a849294",
   "metadata": {},
   "source": [
    "## 2) Manual Tokenization + Model Forward\n",
    "- Definition: A tokenizer maps text to token IDs and attention masks; a model head is a task-specific layer (e.g., classification).\n",
    "- Why: Manual control lets you batch, pad, and inspect outputs precisely for downstream evaluation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fc2be59",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = 'distilbert-base-uncased-finetuned-sst-2-english'\n",
    "tok = AutoTokenizer.from_pretrained(model_id)\n",
    "mdl = AutoModelForSequenceClassification.from_pretrained(model_id)\n",
    "\n",
    "texts = [\n",
    "    'I absolutely loved this movie!',\n",
    "    'The plot was weak and boring.'\n",
    "]\n",
    "batch = tok(texts, padding=True, truncation=True, return_tensors='pt')\n",
    "with torch.no_grad():\n",
    "    out = mdl(**batch)\n",
    "\n",
    "probs = out.logits.softmax(-1)\n",
    "probs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3afd2e36",
   "metadata": {},
   "source": [
    "### Generation with `generate()`\n",
    "- Definition: Decoding chooses next tokens (sampling vs. beam search).\n",
    "- Why: Tuning decoding trades off creativity vs. determinism and repetition.\n",
    "Key parameters: `max_new_tokens`, `temperature`, `top_p`, `top_k`, `num_beams`, `repetition_penalty`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f943b080",
   "metadata": {},
   "outputs": [],
   "source": [
    "lm_id = 'gpt2'  # small demo model\n",
    "tok_lm = AutoTokenizer.from_pretrained(lm_id)\n",
    "lm = AutoModelForCausalLM.from_pretrained(lm_id)\n",
    "inputs = tok_lm('Tell me a joke about the people of Marseille', return_tensors='pt')\n",
    "out = lm.generate(**inputs, max_new_tokens=64, do_sample=True, temperature=0.7, top_p=0.9)\n",
    "print(tok_lm.decode(out[0], skip_special_tokens=True))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8bc0d61",
   "metadata": {},
   "source": [
    "## 3) Devices and Memory\n",
    "- Definition: `device_map=\"auto\"` automatically places layers across CPU/GPU/MPS; `torch_dtype` sets numeric precision; quantization loads 8/4-bit weights.\n",
    "- Why: Fit models in memory and run them faster on your hardware.\n",
    "Use `device_map=\"auto\"` to place weights on available accelerators. Reduce memory via `torch_dtype=torch.float16` or 8/4-bit loading (requires `bitsandbytes`).\n",
    "\n",
    "### Experiment: benchmark dtype & device\n",
    "We compare runtime and memory across supported devices/dtypes. On CPU, float16 compute is usually not supported, so we skip it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "402ea341",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time, torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "name = 'distilbert-base-uncased-finetuned-sst-2-english'\n",
    "texts = ['STUDYING DATA SCIENCE IS FUN!'] * 16\n",
    "\n",
    "devices = []\n",
    "if torch.cuda.is_available(): devices.append('cuda')\n",
    "if getattr(torch.backends, 'mps', None) and torch.backends.mps.is_available():\n",
    "    devices.append('mps')\n",
    "devices.append('cpu')\n",
    "\n",
    "rows = []\n",
    "for device in devices:\n",
    "    for dtype in (torch.float32, torch.float16):\n",
    "        if device == 'cpu' and dtype is torch.float16:\n",
    "            continue\n",
    "        tok = AutoTokenizer.from_pretrained(name)\n",
    "        mdl = AutoModelForSequenceClassification.from_pretrained(name, dtype=dtype)\n",
    "        mdl.to(device)\n",
    "        batch = tok(texts, padding=True, truncation=True, return_tensors='pt')\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        # Estimate parameter memory\n",
    "        param_mb = sum(p.numel() * p.element_size() for p in mdl.parameters()) / 1e6\n",
    "        # Optional: GPU peak memory\n",
    "        if device == 'cuda':\n",
    "            torch.cuda.reset_peak_memory_stats()\n",
    "        # Warmup + timed run\n",
    "        with torch.no_grad(): mdl(**batch)\n",
    "        t0 = time.perf_counter()\n",
    "        with torch.no_grad():\n",
    "            for _ in range(10): mdl(**batch)\n",
    "        dt = time.perf_counter() - t0\n",
    "        peak_mb = None\n",
    "        if device == 'cuda':\n",
    "            peak_mb = torch.cuda.max_memory_allocated() / 1e6\n",
    "        rows.append((device, str(dtype).replace('torch.', ''), round(param_mb,1), round(dt,3), None if peak_mb is None else round(peak_mb,1)))\n",
    "\n",
    "print('device\tdtype\tparamMB\tsec(10 iters)\tpeakMB(cuda)')\n",
    "for r in rows:\n",
    "    print('\t'.join(map(str, r)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f69dfb5c",
   "metadata": {},
   "source": [
    "## 4) Understanding Batch Processing with Hugging Face Transformers\n",
    "The following code demonstrates a complete pipeline for processing and analyzing text data using Hugging Face's transformers library. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69f8911d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import time\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# ...existing imports...\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score\n",
    "import psutil  # for RAM monitoring\n",
    "\n",
    "def measure_memory_cpu():\n",
    "    \"\"\"Get current memory usage in MB\"\"\"\n",
    "    process = psutil.Process()\n",
    "    return process.memory_info().rss / 1024 / 1024\n",
    "\n",
    "def process_with_detailed_metrics(dataset, batch_size):\n",
    "    \"\"\"Process with detailed per-batch metrics\"\"\"\n",
    "    t0 = time.perf_counter()\n",
    "    predictions = []\n",
    "    memory_usage = []\n",
    "    batch_times = []\n",
    "    batch_accuracies = []\n",
    "    \n",
    "    for i in tqdm(range(0, len(dataset), batch_size)):\n",
    "        batch = dataset[i:i + batch_size]\n",
    "        batch_t0 = time.perf_counter()\n",
    "        \n",
    "        # Record memory before batch\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.reset_peak_memory_stats()\n",
    "        mem_before = measure_memory_cpu()\n",
    "        \n",
    "        # Process batch\n",
    "        inputs = tokenizer(batch['sentence'], \n",
    "                         padding=True,\n",
    "                         truncation=True, \n",
    "                         return_tensors='pt').to(model.device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            output = model(**inputs)\n",
    "            preds = output.logits.argmax(-1).cpu().tolist()\n",
    "            predictions.extend(preds)\n",
    "        \n",
    "        # Record metrics\n",
    "        batch_time = time.perf_counter() - batch_t0\n",
    "        batch_times.append(batch_time)\n",
    "        \n",
    "        # Memory delta\n",
    "        mem_after = measure_memory_cpu()\n",
    "        memory_delta = mem_after - mem_before\n",
    "        if torch.cuda.is_available():\n",
    "            memory_delta += torch.cuda.max_memory_allocated() / 1024 / 1024\n",
    "        memory_usage.append(memory_delta)\n",
    "        \n",
    "        # Batch accuracy\n",
    "        batch_acc = accuracy_score(batch['label'], preds)\n",
    "        batch_accuracies.append(batch_acc)\n",
    "    \n",
    "    total_time = time.perf_counter() - t0\n",
    "    \n",
    "    return {\n",
    "        'predictions': predictions,\n",
    "        'total_time': total_time,\n",
    "        'batch_times': batch_times,\n",
    "        'memory_usage': memory_usage,\n",
    "        'batch_accuracies': batch_accuracies,\n",
    "        'avg_batch_time': np.mean(batch_times),\n",
    "        'avg_memory': np.mean(memory_usage),\n",
    "        'avg_accuracy': np.mean(batch_accuracies)\n",
    "    }\n",
    "\n",
    "# Test different batch sizes\n",
    "batch_sizes = [1, 8, 32, 64]\n",
    "results = {}\n",
    "\n",
    "for bs in batch_sizes:\n",
    "    print(f\"\\nProcessing with batch_size={bs}\")\n",
    "    results[bs] = process_with_detailed_metrics(ds, bs)\n",
    "\n",
    "# Print summary\n",
    "print(\"\\nDetailed Comparison:\")\n",
    "print(f\"{'Batch Size':>10} {'Memory (MB)':>12} {'Time (s)':>10} {'Accuracy':>10}\")\n",
    "print(\"-\" * 45)\n",
    "for bs in batch_sizes:\n",
    "    r = results[bs]\n",
    "    print(f\"{bs:>10} {r['avg_memory']:>12.1f} {r['avg_batch_time']:>10.3f} {r['avg_accuracy']:>10.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bf7a67c",
   "metadata": {},
   "source": [
    "## 5) Revisions, Caching, and Offline\n",
    "- Definition: A checkpoint is a released model; a revision is an exact commit/tag.\n",
    "- Why: Pinning revisions and controlling caches ensures reproducibility on different machines.\n",
    "- Pin exact versions with `revision=` when calling `from_pretrained`.\n",
    "- Set cache directories with `HF_HOME` or `TRANSFORMERS_CACHE`.\n",
    "- Force offline mode with `HF_HUB_OFFLINE=1` (uses only local cache).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01c1f243",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: pinning a revision (replace with a real commit SHA/tag for production)\n",
    "tok_pinned = AutoTokenizer.from_pretrained('bert-base-uncased', revision='main')\n",
    "mdl_pinned = AutoModelForSequenceClassification.from_pretrained('distilbert-base-uncased-finetuned-sst-2-english', revision='main')\n",
    "\n",
    "# Where is the cache?\n",
    "print('HF_HOME =', os.getenv('HF_HOME'))\n",
    "print('TRANSFORMERS_CACHE =', os.getenv('TRANSFORMERS_CACHE'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eae6b11",
   "metadata": {},
   "source": [
    "## 6) Optional: Hosted Inference API\n",
    "- Definition: The Inference API is a managed endpoint for common tasks.\n",
    "- Why: Zero setup for quick demos or when you lack local GPU resources.\n",
    "Run inference on HF-hosted models (requires token; rate limits apply).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2da7cdba",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from huggingface_hub import InferenceClient\n",
    "    if HF_TOKEN:\n",
    "        client = InferenceClient(model='facebook/bart-large-cnn', token=HF_TOKEN)\n",
    "        s = client.summarization('''Large Language Models (LLMs) are a transformative technology in artificial intelligence, powering applications like chatbots, text generation, and automated analysis. These models, built on deep learning architectures, excel at understanding and generating human-like text by learning patterns from vast datasets. At their core, LLMs are neural networks trained on billions of words from diverse sources, such as books, websites, and social media, enabling them to capture the nuances of language, from grammar to context.\n",
    "The foundation of LLMs lies in the transformer architecture, introduced in 2017 with the paper \"Attention is All You Need.\" Transformers use mechanisms like self-attention to process input text, allowing the model to weigh the importance of each word relative to others in a sentence. This enables LLMs to handle long-range dependencies, making them adept at tasks like translation, summarization, and question-answering. Models like BERT, GPT, and Llama have pushed the boundaries of what machines can achieve, with each iteration scaling up in size and capability.\n",
    "Training an LLM involves feeding it massive text corpora and optimizing billions of parameters using techniques like supervised learning and reinforcement learning. The process is computationally intensive, requiring powerful GPUs or TPUs and significant energy resources. Once trained, LLMs can perform zero-shot or few-shot learning, meaning they can tackle tasks with little to no task-specific training, relying on their prelearned knowledge. For example, an LLM can generate a story from a prompt like \"Once upon a time\" or classify sentiment in a sentence without explicit retraining.\n",
    "LLMs are accessible through platforms like the Hugging Face Hub, which hosts pretrained models, datasets, and tools like the transformers library. This democratizes AI, allowing developers to use models like GPT-2 or DistilBERT for tasks such as text classification or generation without building from scratch. However, using LLMs responsibly requires understanding their limitations, including biases in training data, high computational costs, and potential security risks when loading untrusted models.\n",
    "Applications of LLMs span industries: they power virtual assistants, automate customer support, assist in coding, and enhance research by summarizing complex texts. Yet, challenges remain, including ensuring fairness, reducing environmental impact, and managing the risk of generating misleading information. As LLMs evolve, they promise to reshape how we interact with technology, making it critical to approach their development and use with care.\n",
    "In summary, LLMs represent a leap forward in AI, driven by transformers and massive datasets. Their ability to process and generate language has broad implications, but careful management is essential to harness their potential effectively..''')\n",
    "        print(s)\n",
    "    else:\n",
    "        print('HF_TOKEN not set; skipping hosted inference.')\n",
    "except Exception as e:\n",
    "    print('InferenceClient not available or error:', e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10869928",
   "metadata": {},
   "source": [
    "## Exercise — Model comparison and benchmarking\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05a52d5f",
   "metadata": {},
   "source": [
    "###  Task Description\n",
    "Create a comprehensive comparison of two sentiment classifiers using the IMDB dataset. Your analysis should include:\n",
    "\n",
    "#### Model Selection\n",
    "- Compare `distilbert-base-uncased-finetuned-sst-2-english` and `textattack/bert-base-uncased-SST-2`\n",
    "- Document the model architectures and sizes\n",
    "\n",
    "#### Implementation Requirements\n",
    "- Use the Hugging Face datasets library to load IMDB data\n",
    "- Implement batch processing for memory efficiency\n",
    "- Include proper error handling and device management\n",
    "- Record and compare inference times\n",
    "\n",
    "#### Evaluation Metrics\n",
    "- Calculate and compare accuracy scores\n",
    "- Generate classification reports\n",
    "- Record processing time per batch\n",
    "- Document memory usage where applicable\n",
    "\n",
    "#### Technical Requirements\n",
    "- Pin model revisions for reproducibility \n",
    "- Record label mappings from `config.id2label`\n",
    "- Use appropriate batch sizes (suggest starting with 32)\n",
    "\n",
    "#### Deliverables\n",
    "- Working code implementation\n",
    "- Performance comparison table\n",
    "- Brief analysis of tradeoffs (speed vs. accuracy)\n",
    "- Documentation of label mappings and any data preprocessing\n",
    "\n",
    "#### Bonus Tasks\n",
    "- Experiment with different dataset sizes\n",
    "- Add visualization of results\n",
    "- Compare memory usage across models\n",
    "- Analyze misclassified examples\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (venvLLMDS)",
   "language": "python",
   "name": "venvllmds"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
